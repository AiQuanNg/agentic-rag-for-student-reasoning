# Agentic RAG for Student Reasoning

An agent-based RAG system for analyzing student answers, classifying them as **Standard**, **Latent**, or **Off-topic**, and refining a
**Thematic Codebook** with professor oversight.\
This combines **retrieval-augmented generation (RAG)**, **rubric-based classification**, and **professor-in-the-loop feedback** for adaptive,
explainable evaluation.

------------------------------------------------------------------------

## ‚ú® Features

-   **Multi-agent workflow** (Orchestrator, Extractor, Classifier, Summary Reporter, Aggregator)
-   **Rubric & Criteria-driven classification** (Standard / Latent / Off-topic)
-   **Professor-in-the-loop approvals** for new themes/keywords
-   **PostgreSQL + pgvector** for semantic retrieval
-   **Sentence-Transformers** (`all-MiniLM-L6-v2`) for efficient, local embeddings
-   **Batch processing** of student submissions with retries & confidence thresholds
-   **Evaluation dashboard** for precision/recall and drift detection

------------------------------------------------------------------------

## üìÇ Project Structure

```
agentic-rag-student-reasoning/
‚îú‚îÄ‚îÄ agent/                  # Core agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py     # ‚úÖ Python-based workflow manager
‚îÇ   ‚îú‚îÄ‚îÄ extractor.py        # üÜï Pydantic AI agent (NEW)
‚îÇ   ‚îú‚îÄ‚îÄ classifier.py       # ‚è≥ Build after Extractor
‚îÇ   ‚îú‚îÄ‚îÄ reporter.py         # ‚úÖ Summary Reporter
‚îÇ   ‚îú‚îÄ‚îÄ aggregator.py       # ‚è≥ Build after Classifier
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tools/              # üÜï Tools Layer (NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ codebook.py     # Keyword retrieval tools
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retrieval.py    # pgvector search utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/            # üÜï Prompt Engineering (NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extractor.py    # EXTRACTOR_SYSTEM_PROMPT
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.py   # (Future)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aggregator.py   # (Future)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ models/             # üÜï Validation Layer (NEW)
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ extraction.py   # ExtractionResult, ExtractorContext
‚îÇ       ‚îú‚îÄ‚îÄ classification.py    # (Future)
‚îÇ       ‚îî‚îÄ‚îÄ common.py            # Shared models
‚îÇ
‚îú‚îÄ‚îÄ config/                  # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ extractor_config.py  # ‚úÖ Extractor configuration
‚îÇ
‚îú‚îÄ‚îÄ scripts/                         # Utility scripts
‚îÇ   ‚îî‚îÄ‚îÄ generate_embeddings.py       # ‚úÖ Generate embeddings for DB
‚îÇ
‚îú‚îÄ‚îÄ data/                        # Data files
‚îÇ   ‚îú‚îÄ‚îÄ questions.csv            # ‚úÖ Questions dataset
‚îÇ   ‚îú‚îÄ‚îÄ rubrics.csv              # ‚úÖ Rubrics dataset
‚îÇ   ‚îú‚îÄ‚îÄ criteria.csv             # ‚úÖ Criteria dataset
‚îÇ   ‚îú‚îÄ‚îÄ topic_keywords.csv       # ‚úÖ Keywords dataset
‚îÇ   ‚îî‚îÄ‚îÄ student_answers.csv      # ‚úÖ Student submissions
‚îÇ
‚îú‚îÄ‚îÄ sql/                      # Database schemas
‚îÇ   ‚îú‚îÄ‚îÄ schema.sql            # ‚úÖ PostgreSQL + pgvector
‚îÇ   ‚îú‚îÄ‚îÄ constraints.sql       # ‚úÖ Database constraints
‚îÇ   ‚îî‚îÄ‚îÄ clean.sql             # ‚úÖ Cleanup scripts
‚îÇ
‚îú‚îÄ‚îÄ tests/                           # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_extractor.py            # üÜï Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_codebook_tools.py       # üÜï Tool tests
‚îÇ   ‚îî‚îÄ‚îÄ test_extraction_models.py    # üÜï Validation tests
‚îÇ
‚îú‚îÄ‚îÄ planning.md           # ÔøΩ Project planning document
‚îú‚îÄ‚îÄ TASK.md               # ÔøΩ Task tracking
‚îú‚îÄ‚îÄ README.md             # ÔøΩ Project documentation
‚îú‚îÄ‚îÄ requirements.txt      # ÔøΩ Python dependencies
‚îî‚îÄ‚îÄ pytest.ini            # ‚öôÔ∏è Pytest configuration
```



## üèóÔ∏è Architecture (Following Ottomator Pattern)

This system adopts a **layered agentic architecture** inspired by production RAG systems:

### Five-Layer Design

#### 1. **User Layer**
- **FastAPI**: Orchestrator endpoints (`POST /runs`, `GET /runs/{id}/stream`)
- **CLI**: Development testing tools

#### 2. **Agent Layer** (LLM Reasoning)
**Pydantic AI Agents**: Type-safe, tool-enabled LLM agents
- **Extractor**: GPT-4o-mini with codebook retrieval tools
- **Classifier**: GPT-4o with rubric retrieval tools (future)
- **Aggregator**: Pattern detection agent (future)

#### 3. **Tools Layer** (Retrieval Functions)
**Codebook Tools**: Keyword retrieval from approved thematic codebook
- `retrieve_codebook_keywords(question_id)` - String-based lookup
- `search_similar_keywords(embedding)` - Optional pgvector search

**Rubric Tools** (future):
- `retrieve_rubric(answer_embedding, top_k)` - pgvector similarity

**Criteria Tools** (future):
- `retrieve_criteria(answer_embedding, top_k)` - pgvector similarity

#### 4. **Database Layer**
**PostgreSQL + pgvector**: Vector similarity search
- `topic_keywords.embedding` - 384-dim (all-MiniLM-L6-v2)
- `rubrics.embedding`, `criteria.embedding`
- **asyncpg**: Async database connection pool

#### 5. **Validation Layer**
**Pydantic Models**: Structured input/output validation
- `ExtractionResult`: Enforces schema compliance for Extractor outputs
- `ClassificationResult`: (future)

### Agentic Loop Pattern

```
User ‚Üí Orchestrator ‚Üí Extractor Agent
                          ‚Üì
              [Tools: retrieve_codebook_keywords]
                          ‚Üì
              PostgreSQL + pgvector
                          ‚Üì
              Structured ExtractionResult
                          ‚Üì
              Classifier Agent (future)
```

**Key Insight**: Unlike LLM-based orchestration, our **Orchestrator uses Python logic** for workflow control (deterministic), while **agents use LLMs** for semantic reasoning (probabilistic).

### Extractor Agent Architecture

```python
# agent/extractor.py
extractor_agent = Agent(
    model=OpenAIModel("gpt-4o-mini"),      # Cost-efficient for extraction
    system_prompt=EXTRACTOR_SYSTEM_PROMPT, # 280+ lines with tool guidance
    result_type=ExtractionResult           # Pydantic validation
)

@extractor_agent.tool
async def retrieve_codebook_keywords(ctx: RunContext) -> List[str]:
    """LLM calls this to get approved keywords"""
    return await db.fetch_keywords(ctx.deps.question_id)

# LLM decides when/how to use tools based on system prompt
result = await extractor_agent.run(
    f"Extract keywords from: {student_answer}",
    deps=ExtractorContext(db_pool=pool, question_id=1)
)
```

### Why Two Models?

| **Component** | **Model** | **Purpose** |
|---------------|-----------|-------------|
| **Embedding** | Sentence-Transformers<br/>(`all-MiniLM-L6-v2`) | Convert text ‚Üí vectors for pgvector similarity |
| **Reasoning** | GPT-4o-mini/GPT-4o<br/>(via Pydantic AI) | Semantic understanding, keyword matching, classification |

**Embeddings are pre-computed** (stored in DB), **LLMs do reasoning** at runtime.

### Classifier Agent Architecture (Hybrid Approach)

**Challenge**: Generic rubrics are not semantically rich enough for pure pgvector retrieval.

**Solution**: Use a hybrid classifier that combines the Extractor's structured output with LLM reasoning and rubric context.

```python
# agent/classifier.py (illustrative example)
from typing import Dict
from pydantic_ai import Agent

# Replace these imports with the project's actual modules
# from agent.models.classification import ClassificationResult
# from agent.contexts import RunContext, ClassifierContext

classifier_agent = Agent(
    model=OpenAIModel("gpt-4o"),
    system_prompt=CLASSIFIER_SYSTEM_PROMPT,
    result_type=ClassificationResult,
)

@classifier_agent.tool
async def get_question_rubrics(ctx: RunContext) -> Dict:
    """Fetch all rubric levels (no similarity filtering)."""
    return await db.fetch_all_rubrics(ctx.deps.question_id)

@classifier_agent.tool
async def get_classification_criteria(ctx: RunContext) -> Dict:
    """Fetch human-readable Standard vs Latent criteria definitions."""
    return await db.fetch_criteria()
```

Classifier uses the Extractor's structured output (keywords, themes, novel terms, confidence) as a decision proxy. Example usage:

```python
# example usage: run the classifier with extractor context
result = await classifier_agent.run(
    f"""Classify answer:
ANSWER: {student_answer}

EXTRACTOR FOUND:
- Keywords: {extraction.matched_keywords}
- Themes: {extraction.detected_themes}
- Novel Terms: {extraction.novel_terms}
- Confidence: {extraction.extraction_confidence}

Determine if STANDARD, LATENT, or OFF_TOPIC.
""",
    deps=ClassifierContext(db_pool=pool, question_id=1),
)
```

**Why Hybrid?**

| Aspect | Pure pgvector | Hybrid Approach |
|---|---:|:---|
| Rubric matching | Low/ambiguous similarity | LLM reads rubric descriptors and judges fit |
| Standard detection | Misses semantic signal | Uses keyword-count proxy and rubric alignment |
| Latent detection | Often conflated with Standard | Uses `novel_terms` + `detected_themes` signals |
| Interpretability | "Why 0.52 similarity?" | Clear decision logic and reasoning traces |

------------------------------------------------------------------------

## üöÄ Quick Start

### 1. Setup Environment

``` bash
# Create virtual environment
python -m venv venv
source venv/bin/activate   # Linux/macOS
venv\Scripts\activate      # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup PostgreSQL

``` bash
# Create database
createdb agentic_rag

# Run schema
psql -d agentic_rag -f sql/schema.sql
```

> ‚ö†Ô∏è Adjust embedding dimensions in `schema.sql` based on your embedding
> model (e.g., 384, 768 or 1536).

### 3. Configure `.env`

``` env
DATABASE_URL=postgresql://username:password@localhost:5432/agentic_rag

LLM_PROVIDER=openai
LLM_API_KEY=sk-your-key
LLM_MODEL=gpt-4.1-mini
EMBEDDING_MODEL=all-MiniLM-L6-v2

APP_ENV=development
LOG_LEVEL=INFO
```

### 4. Ingest Data

``` bash
# Import professor-defined rubric, criteria, keywords, and questions

# Import student answers

```

### 5. Run Batch Processing

``` bash
python cli/cli.py run --question 1 --batch-size 10 --threshold 0.8
```

------------------------------------------------------------------------

## üîÑ Workflow

1.  **Ingestion**: Professors provide questions, rubric, criteria, keywords ‚Üí stored in DB.
2.  **Batch Processing**: Orchestrator splits student answers into batches.
3.  **Agents**:
    -   Extracting Agent ‚Üí themes/keywords
    -   Classifier Agent ‚Üí Standard / Latent / Off-topic + reasoning
    -   Reporter ‚Üí summary
    -   Aggregator ‚Üí novel patterns
4.  **Professor Review**: Review new keywords/themes require approval.
5.  **Outputs**: JSON/CSV reports + aggregated insights.

------------------------------------------------------------------------

## üìä Evaluation

-   Gold set of labeled answers maintained for benchmarking
-   Metrics:
    -   Precision/Recall/F1 per class
    -   Retry rate
    -   Confidence distribution
    -   Keyword drift rate
-   Low-confidence cases prioritized for human review

------------------------------------------------------------------------

## üß™ Testing

``` bash
pytest
pytest --cov=agent --cov=api --cov-report=html
```

------------------------------------------------------------------------

## üîß Troubleshooting

-   **DB errors?** Confirm `DATABASE_URL` in `.env`.
-   **Low performance?** Check embedding dimensions in schema + index setup.

------------------------------------------------------------------------
Built with ‚ù§Ô∏è using **FastAPI**, **PostgreSQL + pgvector**, and **multi-agent orchestration**.
